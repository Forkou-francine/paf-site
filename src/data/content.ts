import heroPhoto from "../assets/ange1.jpg";
import cnafDashboard from "../assets/projects/tb-reclam.jpeg";
import cnafAnalyse from "../assets/projects/project2.png";
import schoolProjectOne from "../assets/projects/project1.png";
import schoolProjectTwo from "../assets/projects/project3.png";
import schoolProjectTwoOne from "../assets/projects/ae.png";
import fintechPipeline from "../assets/projects/fintech.png";
import fintechArchi from "../assets/projects/fintech-archi.png";
import fintechDashboard from "../assets/projects/fintech-dashboard.png";
import fintechTable from "../assets/projects/fintech-table.png";


import logoDatabricks from "../assets/logos/databricks.png";
import logoSpark from "../assets/logos/spark.png";
import logoPython from "../assets/logos/python.png";
import logoPowerBI from "../assets/logos/powerbi.png";
import logoAzure from "../assets/logos/azure.png";
import logoHadoop from "../assets/logos/hadoop.png";
import logoMap from "../assets/logos/map.jpeg";
import logoAirflow from "../assets/logos/airflow.png";
import logoDbt from "../assets/logos/Dbt.png";
import logoDocker from "../assets/logos/Docker.png";
import logoPostgreSQL from "../assets/logos/postgresql.png";
import logoLookerStudio from "../assets/logos/looker-studio.png";

export type LanguageKey = "fr" | "en";

export type SkillItem = { name: string; detail?: string };
export type SkillGroup = { category: string; intro?: string; items: SkillItem[] };

export type NavItem = { to: string; label: string };

export type HomeMetricKey = "projectCount" | "stackCount" | "pipelineCount";
export type HomeMetric = { label: string; valueKey: HomeMetricKey };

export type Labels = {
  nav: {
    items: NavItem[];
    downloadCv: string;
    openMenu: string;
    closeMenu: string;
    language: {
      switchTo: string;
      aria: string;
    };
  };
  home: {
    availability: string;
    primaryCta: string;
    secondaryCta: string;
    metricsTitle: string;
    metricsSubtitle: string;
    aboutTitle: string;
    aboutText: string;
    languagesTitle: string;
    softSkillsTitle: string;
    interestsTitle: string;
    metrics: HomeMetric[];
  };
  experience: {
    title: string;
    subtitle: string;
  };
  projects: {
    title: string;
    subtitle: string;
    showMore: string;
    showLess: string;
    empty: string;
    lightbox: {
      close: string;
      prev: string;
      next: string;
    };
  };
  projectCard: {
    openGallery: string;
    roleLabel: string;
    durationLabel: string;
    stackLabel: string;
    deliverablesLabel: string;
    viewProject: string;
  };
  filterBar: {
    all: string;
    tooltip: string;
  };
  skills: {
    title: string;
    subtitle: string;
  };
  certifications: {
    title: string;
    subtitle: string;
    empty: string;
    view: string;
  };
  education: {
    title: string;
    subtitle: string;
  };
  contact: {
    title: string;
    subtitle: string;
    headline: string;
    intro: string;
    searchTitle: string;
    searchBody: string;
    emailLink: string;
    phoneLink: string;
  };
  contactForm: {
    name: string;
    email: string;
    subject: string;
    subjectPlaceholder: string;
    message: string;
    messagePlaceholder: string;
    submitIdle: string;
    submitSending: string;
    success: string;
    error: string;
    fallback: string;
  };
  footer: {
    madeWith: string;
    rights: string;
  };
  theme: {
    system: string;
    light: string;
    dark: string;
    buttonLabel: string;
  };
};

import type {
  Profile,
  Experience,
  Education,
  Cert,
  Project,
} from "../types/types";

export type PortfolioContent = {
  profile: Profile;
  experiences: Experience[];
  education: Education[];
  certifications: Cert[];
  projects: Project[];
  skills: SkillGroup[];
  labels: Labels;
  pipelinesCount: number;
};

const frenchContent: PortfolioContent = {
  profile: {
    name: "Ange Francine FORKOU",
    title: "Data Engineer & D√©veloppeuse BI",
    tagline: "Je transforme des millions de lignes en tableaux de bord que les gens utilisent vraiment.",
    bio: "En alternance √† la CNAF, je con√ßois les pipelines de donn√©es et dashboards qui aident 101 caisses d'allocations familiales √† piloter leurs r√©clamations. Databricks, PySpark, Power BI, Azure : c'est mon quotidien depuis 2 ans.",
    location: "Ille-et-Vilaine, France",
    email: "francineforkou@gmail.com",
    phone: "+33 6 95 27 78 30",
    linkedin: "https://www.linkedin.com/in/forkou-francine",
    github: "https://github.com/Forkou-francine",
    medium: "https://medium.com/@francineforkou",
    cvUrl: "/CV_Data_Francine_FORKOU.pdf",
    heroPhoto,
    languages: [
      { name: "Fran√ßais", level: "Courant" },
      { name: "Anglais", level: "C1+ (TOEIC 950/990, BULATS 98/100)" },
    ],
    softSkills: [
      "Je sais expliquer - J'ai form√© des utilisateurs m√©tier √† mes dashboards",
      "Je livre - Mes pipelines tournent en prod tous les mois depuis 1 an",
      "Je collabore - 5 ans d√©l√©gu√©e de classe, √ßa laisse des traces",
      "Je traduis ‚Äî Les besoins m√©tier en requ√™tes SQL et dashboards lisibles",
    ],
    interests: ["Technologie & veille data", "Entrepreneuriat", "B√©n√©volat"],
  },
  experiences: [
    {
      company: "Caisse Nationale des Allocations Familiales (CNAF)",
      location: "Rennes, FR",
      title: "Alternance - Data Engineer / D√©veloppeuse d'applicatifs BI",
      period: "Sept. 2024 - Sept. 2026",
      bullets: [
        "Refonte du syst√®me national de suivi des r√©clamations : conception et d√©ploiement d'un tableau de bord Power BI utilis√© par 101 CAF et 500 utilisateurs, en remplacement d'une application SAS obsol√®te.",
        "D√©veloppement de pipelines ETL sous Databricks (PySpark) traitant plusieurs millions de r√©clamations/an avec une architecture Medallion (Bronze/Silver/Gold).",
        "Orchestration de 6 notebooks via Databricks Workflows avec un temps d'ex√©cution optimis√© √† 7 minutes en batch mensuel.",
        "Impl√©mentation de la gouvernance des donn√©es via Unity Catalog et stockage sur Delta Lake / Azure Storage.",
        // "Cr√©ation d'un tableau de bord pour le pilotage des Services d'Aide √† Domicile, consolidant les donn√©es de financeurs multiples (CAF, D√©partements, CPAM).",
        "Revue et standardisation de notebooks Python/Spark existants : am√©lioration des performances et respect des bonnes pratiques.",
        "R√©alisation d'un POC Databricks AI/BI (Genie Room) : √©valuation des capacit√©s de requ√™tage en langage naturel, recommandations adopt√©es par l'√©quipe.",
        "Collaboration avec les √©quipes MOA/AMOA ; versioning et maintenance des pipelines via Git (Azure DevOps).",
      ],
      tech: ["Databricks", "Python", "Spark", "Power BI", "Azure", "Unity Catalog", "Delta Lake", "Azure DevOps", "Git"],
    },
    {
      company: "Orange",
      location: "Cesson-S√©vign√©, FR",
      title: "Stage - Ing√©nieur Logiciel",
      period: "F√©v. 2024 - Ao√ªt 2024",
      bullets: [
        "D√©veloppement d'une solution de visualisation de donn√©es r√©seaux de transport optique : API Python (FastAPI) et TypeScript (Vue.js) pour cr√©er des dashboards interactifs.",
        "Administration de bases de donn√©es NoSQL (MongoDB) sur clusters Kubernetes (Linux).",
        "Mise en place de pipelines CI/CD via GitLab CI et SonarQube pour le contr√¥le qualit√©.",
      ],
      tech: ["FastAPI", "Vue.js", "TypeScript", "MongoDB", "Kubernetes", "GitLab CI", "SonarQube"],
    },
  ],
  education: [
    {
      school: "EPSI, Rennes",
      degree: "Mast√®re sp√©cialis√© - Expert en ing√©nierie des donn√©es",
      period: "Sept. 2024 - Pr√©sent",
      details: "Pipelines de donn√©es, Big Data, Data Viz, Gouvernance, Green IT",
    },
    {
      school: "IUSJ - UTT",
      degree: "Dipl√¥me d‚Äôing√©nieur - D√©veloppement logiciel",
      period: "2019 - 2024",
      details: "Architecture des SI, Mod√©lisation et Data viz (Power BI)",
    },
    {
      school: "UTBM, Belfort",
      degree: "Semestre d‚Äô√©change - D√©veloppement logiciel avanc√©",
      period: "Sept. 2023 - F√©v. 2024",
      details: "Bases de donn√©es (SQL, NoSQL), Gestion de projet Agile, Java",
    },
  ],
  certifications: [
    {
      name: "IBM Data Analyst Professional Certificate",
      issuer: "IBM / Coursera",
      link: "https://www.coursera.org/account/accomplishments/specialization/WQ2GLB46L5M6",
    },
    {
      name: "Dataiku Core Designer",
      issuer: "Dataiku",
      link: "http://verify.skilljar.com/c/z6p5zxzj6ped",
    },
    {
      name: "Microsoft Certified: Power BI Data Analyst Associate",
      issuer: "Microsoft",
      link: "https://learn.microsoft.com/certifications/power-bi-data-analyst-associate/",
    },
    {
      name: "Databricks for Data Engineering",
      issuer: "Databricks",
      link: "https://partner-academy.databricks.com/learn/courses/2469/get-started-with-databricks-for-data-engineering?hash=a4c6e6e8910eb43f6d827a874387454de05a5584&generated_by=880806",
    },
  ],
  projects: [
    {
      name: "Tableau de bord des r√©clamations",
      org: "CNAF - Alternance",
      role: "Data Engineer et r√©f√©rente BI",
      period: "2024 - 2025",
      summary:
        "J'ai repris un syst√®me vieillissant sous SAS pour le reconstruire sur Databricks + Power BI. Aujourd'hui, 101 CAF l'utilisent chaque mois.",
      bullets: [
        "Reconstruction du data mart sur Delta Lake avec automatisation des jobs Databricks",
        "Mod√©lisation et publication d'un espace Power BI avec des mesures DAX r√©utilisables",
        "Documentation et formation des utilisateurs finaux",
      ],
      cover: cnafDashboard,
      gallery: [cnafDashboard, cnafAnalyse],
      stack: ["Databricks", "Spark", "Python", "Power BI"],
      link: cnafDashboard,
      videoUrl: "https://www.loom.com/share/b2131ab6daba43808221329698323a9e"
    },
    {
      name: "Pipeline de qualit√© des donn√©es Fintech",
      org: "Projet personnel",
      role: "Data Engineer",
      period: "2025",
      summary:
        "Pipeline end-to-end qui ing√®re des transactions financi√®res simul√©es, les transforme avec dbt, valide leur qualit√© avec Great Expectations, orchestre le tout avec Airflow, et expose un dashboard de monitoring.",
      bullets: [
        "G√©n√©ration de 10 000 transactions synth√©tiques avec anomalies contr√¥l√©es (2% NULL, 3% suspects, 0.5% fraudes)",
        "Mod√©lisation dbt en 3 couches (staging, intermediate, marts) avec tests de qualit√© d√©claratifs",
        "7 r√®gles de validation Great Expectations couvrant compl√©tude, coh√©rence et anomalies",
        "Orchestration Airflow via Docker : DAG quotidien de 4 t√¢ches s√©quentielles",
        "Dashboard Streamlit interactif avec score de qualit√© global et drill-down par cat√©gorie",
      ],
      cover: fintechDashboard,
      gallery: [fintechDashboard, fintechTable, fintechPipeline, fintechArchi],
      stack: ["Python", "Dbt", "Airflow", "DuckDB", "Streamlit", "Docker"],
      link: "https://github.com/Forkou-francine/fintech-data-quality-pipeline",
    },
    {
      name: "Analyse de donn√©es e-commerce",
      org: "EPSI - M1",
      role: "Data Analyst projet de groupe",
      period: "2024 - 2025",
      summary:
        "Projet de groupe : construction d'un data lake Hadoop pour analyser les ventes d'une plateforme e-commerce et proposer des recommandations marketing.",
      bullets: [
        "Ingestion et pr√©paration des donn√©es avec Hadoop, Spark et MapReduce",
        "Formalisation des KPIs ventes et recommandations dans un playbook marketing",
        "Restitution via rapport interactif et soutenance",
      ],
      cover: schoolProjectOne,
      stack: ["Hadoop", "Spark"],
      link: "https://www.canva.com/design/DAGfjcpth5M/lAgCMBD_27AWh9dLE6yY7g/edit?utm_content=DAGfjcpth5M&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton",
    },
    {
      name: "Pipeline Big Data pour un labo",
      org: "EPSI - M1",
      role: "Lead Data Engineer",
      period: "2024 - 2025",
      summary:
        "Cr√©ation d'un pipeline de bout en bout pour consolider les donn√©es R&D d'un laboratoire, de l'ingestion √† la visualisation dans Looker Studio.",
      bullets: [
        "Architecture de l'orchestration Airflow et transformations dbt/Spark",
        "Mod√©lisation de la base PostgreSQL et s√©curisation des exports Looker Studio",
        "Documentation vivante et CI l√©g√®re sur GitLab",
      ],
      cover: schoolProjectTwo,
      gallery: [schoolProjectTwo, schoolProjectTwoOne],
      stack: ["Python", "Spark", "Airflow", "LookerStudio", "Dbt"],
      link: "https://www.canva.com/design/DAGqzNY5xKY/9o3BTlrJ3Ax-Hc3CJfh6rw/edit",
    },
  ],
  skills: [
    {
      category: "Solutions livr√©es",
      intro: "Ce que je mets en production pour les √©quipes m√©tier.",
      items: [
        {
          name: "Tableaux de bord Power BI",
          detail: "Du cadrage KPI √† la publication et √† la gouvernance des espaces.",
        },
        {
          name: "Pipelines Databricks",
          detail: "Jobs planifi√©s, qualit√© de donn√©es et monitoring.",
        },
        {
          name: "Data marts analytiques",
          detail: "Mod√©lisation dimensionnelle et architecture Medallion",
        },
        {
          name: "Formation utilisateurs",
          detail: "Ateliers et documentation pour ~100 utilisateurs m√©tier",
        },
      ],
    },
    {
      category: "Technologies et outils",
      intro: "Les stacks que je pilote au quotidien.",
      items: [
        { name: "Databricks", detail: "Delta Lake, notebooks collaboratifs et jobs assur√©s." },
        { name: "Spark", detail: "Optimisation des transformations batch et streaming." },
        { name: "Python", detail: "Mon langage principal : scripts, notebooks, APIs" },
        { name: "Power BI", detail: "Mod√©lisation DAX, Power Query; Je con√ßois des dashboards utilis√©s par des centaines de personnes" },
        { name: "Airflow", detail: "Orchestration CI/CD et senseurs multi-sources -  utilis√© sur un projet acad√©mique" },
        { name: "Dbt", detail: "En cours d'apprentissage pour mes projets perso" },
        { name: "Azure", detail: "Storage, Data Factory et DevOps pour le monitoring." },
        { name: "Docker", detail: "Environnements reproductibles pour notebooks et APIs." },
        { name: "Hadoop", detail: "Utilis√© en contexte acad√©mique (HDFS, √©cosyst√®me)" },
      ],
    },
  ],
  labels: {
    nav: {
      items: [
        { to: "/", label: "Accueil" },
        { to: "/experience", label: "Exp√©riences" },
        { to: "/projects", label: "Projets" },
        { to: "/skills", label: "Comp√©tences" },
        { to: "/education", label: "Formation" },
        { to: "/contact", label: "Contact" },
      ],
      downloadCv: "Mon CV",
      openMenu: "Ouvrir le menu",
      closeMenu: "Fermer le menu",
      language: {
        switchTo: "EN",
        aria: "Passer le site en anglais",
      },
    },
    home: {
      availability: "√Ä l‚Äô√©coute d‚Äôopportunit√©s",
      primaryCta: "Voir mes projets",
      secondaryCta: "Me contacter",
      metricsTitle: "Ce que j'ai livr√©",
      metricsSubtitle: "Quelques r√©sultats concrets.",
      aboutTitle: "√Ä propos de moi",
      aboutText: "Camerounaise, ing√©nieure, et passionn√©e de donn√©es. J'ai grandi √† Yaound√©, √©tudi√© l'informatique entre le Cameroun et la France ( Belfort, Rennes), et d√©couvert la data un peu par hasard ‚Äî lors d'un cours de Data Mining en 3√®me ann√©e. Ce qui m'a accroch√©e : le c√¥t√© concret. Quand je livre un dashboard, je vois des gens s'en servir pour prendre des d√©cisions. C'est cette boucle \"donn√©es ‚Üí d√©cision ‚Üí impact\" qui me motive.",
      languagesTitle: "Langues",
      softSkillsTitle: "Atouts",
      interestsTitle: "Centres d‚Äôint√©r√™t",
      metrics: [
        { label: "Projets r√©alis√©s", valueKey: "projectCount" },
        { label: "Technologies utilis√©es", valueKey: "stackCount" },
        { label: "Pipelines de donn√©es", valueKey: "pipelineCount" },
      ],
    },
    experience: {
      title: "Exp√©riences professionnelles",
      subtitle: "Les missions cl√©s qui ont fa√ßonn√© mon expertise.",
    },
    projects: {
      title: "Projets",
      subtitle: "Une s√©lection de mes r√©alisations.",
      showMore: "Voir plus de projets",
      showLess: "Voir moins de projets",
      empty: "Aucun projet ne correspond aux filtres s√©lectionn√©s.",
      lightbox: {
        close: "Fermer",
        prev: "Image pr√©c√©dente",
        next: "Image suivante",
      },
    },
    projectCard: {
      openGallery: "Ouvrir la galerie du projet",
      roleLabel: "R√¥le",
      durationLabel: "Dur√©e",
      stackLabel: "Stack",
      deliverablesLabel: "Livrables cl√©s",
      viewProject: "Voir le projet",
    },
    filterBar: {
      all: "Tout",
      tooltip: "Filtrer : {{tech}}",
    },
    skills: {
      title: "Comp√©tences",
      subtitle: "Ce que je livre et les outils que j'utilise.",
    },
    certifications: {
      title: "Certifications",
      subtitle: "Preuves de mon apprentissage continu.",
      empty: "Aucune certification renseign√©e pour le moment.",
      view: "Voir le certificat",
    },
    education: {
      title: "Formation",
      subtitle: "Mes fondations acad√©miques.",
    },
    contact: {
      title: "Contact",
      subtitle: "Parlons data, produits et impact concret.",
      headline: "Discutons de vos projets",
      intro:
        "J‚Äôaime co-construire des produits data utiles et pilotables. Une id√©e de dashboard, un pipeline √† fiabiliser ou une question sur la gouvernance de la donn√©e ? Je serais ravie d‚Äôen discuter.",
      searchTitle: "Ce que je cherche",
      searchBody:
        "Un CDI en Data Engineering ou BI √† partir de novembre 2026, id√©alement √† Lyon. En attendant, ouverte aux ateliers d'acc√©l√©ration sur vos pipelines et tableaux de bord.",
      emailLink: "√âcrire un email",
      phoneLink: "Appeler",
    },
    contactForm: {
      name: "Nom",
      email: "Email",
      subject: "Sujet (optionnel)",
      subjectPlaceholder: "Sujet",
      message: "Message",
      messagePlaceholder: "Votre message...",
      submitIdle: "Envoyer",
      submitSending: "Envoi...",
      success: "Message envoy√© avec succ√®s !",
      error: "Une erreur est survenue.",
      fallback: "Impossible d‚Äôenvoyer le message.",
    },
    footer: {
      madeWith: "Fait avec du caf√© üòä, React et Tailwind CSS.",
      rights: "¬© {{year}} {{name}}. Tous droits r√©serv√©s.",
    },
    theme: {
      system: "Syst√®me",
      light: "Clair",
      dark: "Sombre",
      buttonLabel: "Changer le th√®me ‚Äî actuel : {{label}}",
    },
  },
  pipelinesCount: 5,
};

const englishContent: PortfolioContent = {
  profile: {
    name: "Ange Francine FORKOU",
    title: "Data Engineer & BI Developer",
    tagline: "I turn millions of rows into dashboards people actually use.",
    bio: "Currently in a work-study program at CNAF, I design data pipelines and dashboards that help 101 French family allowance offices manage their complaints. Databricks, PySpark, Power BI, Azure ‚Äî that's been my daily stack for 2 years. And I document everything I learn along the way.",
    location: "Ille-et-Vilaine, France",
    email: "francineforkou@gmail.com",
    phone: "+33 6 95 27 78 30",
    linkedin: "https://www.linkedin.com/in/forkou-francine",
    github: "https://github.com/Forkou-francine",
    medium: "https://medium.com/@francineforkou",
    cvUrl: "/cv-ange-francine-forkou.pdf",
    heroPhoto,
    languages: [
      { name: "French", level: "Native" },
      { name: "English", level: "C1+ (TOEIC 950/990, BULATS 98/100)" },
    ],
    softSkills: [
      "I translate ‚Äî Business needs into SQL queries and readable dashboards",
      "I dig deep ‚Äî I don't let go of a bug until I understand the why",
      "I deliver ‚Äî My pipelines have been running in prod monthly for 1 year",
      "I share ‚Äî I've trained 100 business users on my tools",
    ],
    interests: ["Technology & data trends", "Entrepreneurship", "Volunteering"],
  },
  experiences: [
    {
      company: "Caisse Nationale des Allocations Familiales (CNAF)",
      location: "Rennes, FR",
      title: "Apprenticeship - Data Engineer / BI Application Developer",
      period: "Sept. 2024 - Sept. 2026",
      bullets: [
        "Complete overhaul of the national complaints tracking system: replaced an aging SAS application with a Power BI dashboard used by 101 CAF offices and 500+ users",
        "Developed ETL pipelines on Databricks (PySpark) ‚Äî millions of complaints processed yearly, Medallion architecture (Bronze/Silver/Gold)",
        "Orchestrated 4 notebooks with Databricks Workflows: monthly execution optimized to 5-7 minutes",
        "Implemented data governance: Unity Catalog for access control, Delta Lake for storage, Azure Storage for persistence",
        // "Created a second dashboard to monitor Home Care Services (SAAD), consolidating data from multiple funders (CAF, Departments, CPAM)",
        "Trained business users on dashboard adoption ‚Äî workshops, documentation, support",
        "Reviewed and standardized existing Python/Spark notebooks: performance optimization, best practices",
        "Databricks AI/BI POC (Genie Room): tested natural language querying capabilities, wrote recommendations adopted by the team",
        "Daily collaboration with MOA/AMOA teams; Git versioning via Azure DevOps",
      ],
      tech: ["Databricks", "Python", "Spark", "Power BI", "Azure", "Unity Catalog", "Delta Lake", "Azure DevOps", "Git"],
    },
    {
      company: "Orange",
      location: "Cesson-S√©vign√©, FR",
      title: "Internship - Software Engineer",
      period: "Feb. 2024 - Aug. 2024",
      bullets: [
        "Developed a data visualization app for optical transport networks: Python API (FastAPI) backend, TypeScript (Vue.js) frontend",
        "Administered MongoDB databases on Kubernetes clusters in a Linux environment",
        "Set up CI/CD pipelines with GitLab CI and quality control via SonarQube",
      ],
      tech: ["FastAPI", "Vue.js", "TypeScript", "MongoDB", "Kubernetes", "GitLab CI", "SonarQube"],
    },
  ],
  education: [
    {
      school: "EPSI, Rennes",
      degree: "Master's degree - Data Engineering Expert",
      period: "Sept. 2024 - Present",
      details: "Data pipelines, Big Data, Data Viz, Governance, Green IT",
    },
    {
      school: "IUSJ (Cameroon) √ó UTT (France)",
      degree: "Engineering degree - Software Development",
      period: "2019 - 2024",
      details: "Information systems architecture, data visualisation (Power BI)",
    },
    {
      school: "UTBM, Belfort",
      degree: "Exchange semester - Advanced Software Development",
      period: "Sept. 2023 - Feb. 2024",
      details: "Databases (SQL, NoSQL), Agile Project Management, Java",
    },
  ],
  certifications: frenchContent.certifications,
  projects: [
    {
      name: "National Complaints Dashboard",
      org: "CNAF - Apprenticeship",
      role: "Data Engineer & BI Lead",
      period: "2024 - 2025",
      summary:
        "I took over an aging SAS system and rebuilt it entirely on Databricks + Power BI. Today, 101 CAF offices use it every month to track their complaints.",
      bullets: [
        "Rebuilt the data mart on Delta Lake with automated Databricks jobs",
        "Modeled and published a Power BI workspace with reusable DAX measures",
        "Documentation and end-user training",
      ],
      cover: cnafDashboard,
      gallery: [cnafDashboard, cnafAnalyse],
      stack: ["Databricks", "Spark", "Python", "Power BI"],
      link: cnafDashboard,
    },
    {
      name: "Fintech Data Quality Pipeline",
      org: "Personal project",
      role: "Data Engineer",
      period: "2025",
      summary:
        "End-to-end pipeline that ingests simulated financial transactions, transforms them with dbt, validates data quality with Great Expectations, orchestrates everything with Airflow, and exposes a monitoring dashboard.",
      bullets: [
        "Generated 10,000 synthetic transactions with controlled anomalies (2% NULL, 3% suspicious, 0.5% fraud)",
        "dbt modeling across 3 layers (staging, intermediate, marts) with declarative quality tests",
        "7 Great Expectations validation rules covering completeness, consistency, and anomalies",
        "Airflow orchestration via Docker: daily DAG with 4 sequential tasks",
        "Interactive Streamlit dashboard with global quality score and category drill-down",
      ],
      cover: fintechPipeline,
      stack: ["Python", "Dbt", "Airflow", "DuckDB", "Streamlit", "Docker"],
      link: "https://github.com/Forkou-francine/fintech-data-quality-pipeline",
    },
    {
      name: "E-commerce Data Analysis",
      org: "EPSI - Big Data Workshop",
      role: "Data Analyst (team project)",
      period: "2024 - 2025",
      summary:
        "Team project: built a Hadoop data lake to analyze e-commerce sales and propose marketing recommendations.",
      bullets: [
        "Data ingestion and preparation with Hadoop, Spark, and MapReduce",
        "Formalized sales KPIs and recommendations in a marketing playbook",
        "Delivered via interactive report and presentation",
      ],
      cover: schoolProjectOne,
      stack: ["Hadoop", "Spark"],
      link: "https://www.canva.com/design/DAGfjcpth5M/lAgCMBD_27AWh9dLE6yY7g/edit?utm_content=DAGfjcpth5M&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton",
    },
    {
      name: "Big Data Pipeline for an R&D Lab",
      org: "EPSI - M1",
      role: "Lead Data Engineer",
      period: "2024 - 2025",
      summary:
        "Built an end-to-end pipeline to consolidate R&D data for a lab, from ingestion to Looker Studio visualization.",
      bullets: [
        "Designed Airflow orchestration and dbt/Spark transformations",
        "Modeled PostgreSQL database and secured Looker Studio exports",
        "Living documentation and lightweight CI on GitLab",
      ],
      cover: schoolProjectTwo,
      gallery: [schoolProjectTwo, schoolProjectTwoOne],
      stack: ["Python", "Spark", "Airflow", "LookerStudio", "Dbt"],
      link: "https://www.canva.com/design/DAGqzNY5xKY/9o3BTlrJ3Ax-Hc3CJfh6rw/edit",
    },
  ],
  skills: [
    {
      category: "Delivered solutions",
      intro: "What I ship and maintain for business teams.",
      items: [
        {
          name: "Power BI dashboards",
          detail: "From KPI scoping to workspace publishing and governance ‚Äî dashboards used by hundreds of people",
        },
        {
          name: "Databricks pipelines",
          detail: "Scheduled jobs, data quality and SLA monitoring on Spark/Python.",
        },
        {
          name: "Analytical data marts",
          detail: "Dimensional modelling and SQL/API exposure for stakeholders.",
        },
        {
          name: "CI/CD automations",
          detail: "Azure DevOps and GitLab workflows to secure releases.",
        },
      ],
    },
    {
      category: "Technologies & tooling",
      intro: "Stacks I operate every day.",
      items: [
        { name: "Databricks", detail: "Delta Lake, collaborative notebooks, and dependable jobs." },
        { name: "Spark", detail: "Optimising batch and streaming transformations." },
        { name: "Python", detail: "Dataframes, PySpark testing, and packaging best practices." },
        { name: "Power BI", detail: "DAX modelling, Power Query, and dynamic parameters." },
        { name: "Airflow", detail: "Used on an academic project ‚Äî still learning" },
        { name: "Dbt", detail: "Currently learning on personal projects"},
        { name: "Azure", detail: "Storage, Data Factory, and DevOps monitoring." },
        { name: "Docker", detail: "Reproducible environments for notebooks and APIs." },
        { name: "Hadoop", detail: "Distributed processing for large volumes." },
      ],
    },
  ],
  labels: {
    nav: {
      items: [
        { to: "/", label: "Home" },
        { to: "/experience", label: "Experience" },
        { to: "/projects", label: "Projects" },
        { to: "/skills", label: "Skills" },
        { to: "/education", label: "Education" },
        { to: "/contact", label: "Contact" },
      ],
      downloadCv: "Download CV",
      openMenu: "Open menu",
      closeMenu: "Close menu",
      language: {
        switchTo: "FR",
        aria: "Switch the site to French",
      },
    },
    home: {
      availability: "Open to opportunities",
      primaryCta: "View my projects",
      secondaryCta: "Get in touch",
      metricsTitle: "What I've delivered",
      metricsSubtitle: "A few results that illustrate what I deliver.",
      aboutTitle: "About me",
      aboutText: "Cameroonian, engineer, and passionate about data. I grew up in Yaound√©, studied computer science between Cameroon and France (Belfort, Rennes), and discovered data almost by accident ‚Äî during a Data Mining course in my third year. What hooked me: the tangible impact. When I deliver a dashboard, I see people use it to make decisions. That \"data ‚Üí decision ‚Üí impact\" loop is what drives me.",
      languagesTitle: "Languages",
      softSkillsTitle: "How I work",
      interestsTitle: "Interests",
      metrics: [
        { label: "Projects delivered", valueKey: "projectCount" },
        { label: "The tools I operate on", valueKey: "stackCount" },
        { label: "Data pipelines", valueKey: "pipelineCount" },
      ],
    },
    experience: {
      title: "Professional experience",
      subtitle: "Key assignments that shaped my expertise.",
    },
    projects: {
      title: "Projects",
      subtitle: "A selection of my work.",
      showMore: "Show more projects",
      showLess: "Show fewer projects",
      empty: "No project matches the selected filters.",
      lightbox: {
        close: "Close",
        prev: "Previous image",
        next: "Next image",
      },
    },
    projectCard: {
      openGallery: "Open the project gallery",
      roleLabel: "Role",
      durationLabel: "Duration",
      stackLabel: "Stack",
      deliverablesLabel: "Key deliverables",
      viewProject: "View project",
    },
    filterBar: {
      all: "All",
      tooltip: "Filter: {{tech}}",
    },
    skills: {
      title: "Skills",
      subtitle: "What I deliver and the tools I use.",
    },
    certifications: {
      title: "Certifications",
      subtitle: "Proof of continuous learning.",
      empty: "No certification listed yet.",
      view: "View certificate",
    },
    education: {
      title: "Education",
      subtitle: "My academic foundations.",
    },
    contact: {
      title: "Contact",
      subtitle: "Let's talk data, products, and measurable impact.",
      headline: "Let's discuss your projects",
      intro:
        "I love co-building data products that are useful. Need a dashboard, a reliable pipeline, or a question about data governance? I'd be happy to discuss.",
      searchTitle: "What I'm looking for",
      searchBody:
        "A permanent Data Engineering or BI position starting September 2026, ideally in Lyon. In the meantime, open to acceleration workshops on your pipelines and dashboards.",
      emailLink: "Send an email",
      phoneLink: "Call",
    },
    contactForm: {
      name: "Name",
      email: "Email",
      subject: "Subject (optional)",
      subjectPlaceholder: "Subject",
      message: "Message",
      messagePlaceholder: "Your message...",
      submitIdle: "Send",
      submitSending: "Sending...",
      success: "Message successfully sent!",
      error: "Something went wrong.",
      fallback: "Unable to send the message.",
    },
    footer: {
      madeWith: "Powered by coffee üòä, React and Tailwind CSS.",
      rights: "¬© {{year}} {{name}}. All rights reserved.",
    },
    theme: {
      system: "System",
      light: "Light",
      dark: "Dark",
      buttonLabel: "Switch theme ‚Äî current: {{label}}",
    },
  },
  pipelinesCount: 5,
};

const contentByLanguage: Record<LanguageKey, PortfolioContent> = {
  fr: frenchContent,
  en: englishContent,
};

export function getContent(language: LanguageKey): PortfolioContent {
  return contentByLanguage[language] ?? contentByLanguage.fr;
}

export const colors = {
  primaryFrom: "from-violet-600",
  primaryTo: "to-indigo-600",
  softFrom: "from-fuchsia-50",
  softTo: "to-indigo-50",
  ring: "focus:ring-violet-600",
};

export const techLogos: Record<string, string> = {
  Databricks: logoDatabricks,
  Spark: logoSpark,
  Python: logoPython,
  "Power BI": logoPowerBI,
  Azure: logoAzure,
  Hadoop: logoHadoop,
  MapReduce: logoMap,
  Airflow: logoAirflow,
  Dbt: logoDbt,
  Docker: logoDocker,
  PostgreSQL: logoPostgreSQL,
  LookerStudio: logoLookerStudio,
};
